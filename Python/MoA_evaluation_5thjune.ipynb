{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import partial, reduce\n",
    "import time\n",
    "from scipy.stats import mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dir = '/Users/habbasi/Desktop/'\n",
    "proj_dir = 'TA'\n",
    "batch= 'SIGMA2_Pilot_2013_10_11'\n",
    "metadata_dfpath = '/Users/habbasi/Desktop/TA/input/metadata_TA.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/habbasi/Desktop/TA/workspace/backend/SIGMA2_Pilot_2013_10_11/41744/41744_normalized.csv',\n",
       " '/Users/habbasi/Desktop/TA/workspace/backend/SIGMA2_Pilot_2013_10_11/41754/41754_normalized.csv',\n",
       " '/Users/habbasi/Desktop/TA/workspace/backend/SIGMA2_Pilot_2013_10_11/41755/41755_normalized.csv',\n",
       " '/Users/habbasi/Desktop/TA/workspace/backend/SIGMA2_Pilot_2013_10_11/41756/41756_normalized.csv',\n",
       " '/Users/habbasi/Desktop/TA/workspace/backend/SIGMA2_Pilot_2013_10_11/41757/41757_normalized.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class load_data:\n",
    "    \n",
    "    def __init__(self,top_dir,proj_dir, batch):\n",
    "        \n",
    "        self.top_dir = top_dir\n",
    "        self.proj_dir = proj_dir\n",
    "        self.batch = batch\n",
    "    \n",
    "    def feat_list(self):    \n",
    "        path = os.path.join(self.top_dir, self.proj_dir, 'input', 'feature_list.txt')\n",
    "        flist = np.loadtxt(str(path), dtype=str)\n",
    "        return flist\n",
    "    \n",
    "    def plates_list(self):\n",
    "        path = os.path.join(self.top_dir, self.proj_dir, \"input\", 'processed_plates_TA.txt')\n",
    "        plist = np.loadtxt(str(path), dtype=str)\n",
    "        return plist\n",
    "    \n",
    "    def sqlpath(self):\n",
    "        \n",
    "        path = os.path.join(self.top_dir, self.proj_dir, 'workspace', 'backend', self.batch)\n",
    "        spath = []\n",
    "    \n",
    "        for folder, sub, files in os.walk(path):    \n",
    "            for f in files:\n",
    "                if 'sqlite' in f:    \n",
    "                    spath.append(os.path.abspath(os.path.join(folder, f)))\n",
    "        return spath\n",
    "    \n",
    "    def filepath(self):\n",
    "        path = os.path.join(self.top_dir, self.proj_dir, 'workspace', 'backend', self.batch)\n",
    "        fpath = []\n",
    "    \n",
    "        for folder, sub, files in os.walk(path):    \n",
    "            for f in files:\n",
    "                if 'normalized' in f:    \n",
    "                    fpath.append(os.path.abspath(os.path.join(folder, f)))\n",
    "        return fpath\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "path = load_data(top_dir, proj_dir, batch)   \n",
    "sql= sorted(path.sqlpath())\n",
    "csv = sorted(path.filepath())\n",
    "featlist = path.feat_list()\n",
    "platelist = path.plates_list()\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite_connect(path):\n",
    "    \n",
    "    conn = sqlite3.connect(path)\n",
    "    image = pd.read_sql_query(\"select *from Image\", conn)\n",
    "    cells = pd.read_sql_query(\"select  * from Cells\", conn)\n",
    "    cyto= pd.read_sql_query(\"select * from Cytoplasm\", conn)\n",
    "    nuclei= pd.read_sql_query(\"select * from Nuclei\", conn)\n",
    "    dt = reduce(lambda x,y: pd.merge(x,y, on=[\"TableNumber\", \"ImageNumber\", \"ObjectNumber\"], how='left'), [cells, nuclei, cyto])\n",
    "    df = reduce(lambda x,y: pd.merge(x,y, on=[\"TableNumber\", \"ImageNumber\"], how='left'), [dt, image])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_func(population, strata, variables, operation):\n",
    "\n",
    "    dt = pd.concat([population[strata], population[variables]], axis=1)   \n",
    "    if (operation == 'mean'):    \n",
    "        tmp = (dt.groupby(strata)[variables]\n",
    "                .apply(lambda x: np.mean(x))\n",
    "                .reset_index()\n",
    "               )\n",
    "        \n",
    "    elif (operation == 'median'): \n",
    "        tmp = (dt.groupby(strata)[variables]\n",
    "               .median()\n",
    "               .reset_index())     \n",
    "    else:\n",
    "        print(\"No operation defined\")\n",
    "        \n",
    "\n",
    "    return tmp\n",
    "    \n",
    "plates = ['41744', '41754', '41755', '41756', '41757']\n",
    "\n",
    "\n",
    "def combined_profiles():\n",
    "    combined = []\n",
    "    for p, cpath, spath in zip(plates, csv, sql):\n",
    "        d = pd.read_csv(str(cpath))\n",
    "        meta = [col for col in d.columns if \"Meta\" in col]\n",
    "        pmeta = d.loc[:, meta]\n",
    "        sql_data = sqlite_connect(str(spath))\n",
    "        imagecol = ['Image_Metadata_Well', 'Image_Metadata_Plate']\n",
    "        data = reduce(lambda x, y: pd.merge(x, y, left_on = [\"Image_Metadata_Plate\",\"Image_Metadata_Well\"],\n",
    "                                            right_on= [\"Metadata_Plate\", \"Metadata_Well\"], how='left'), [sql_data, pmeta])\n",
    "    \n",
    "        controls = data.query('Metadata_ASSAY_WELL_ROLE == \"Untreated\"')\n",
    "        scaler = StandardScaler().fit(controls[featlist])\n",
    "        df_scaled = pd.DataFrame(scaler.transform(data[featlist]), columns=featlist)\n",
    "        metadata = [col for col in data.columns if col.startswith('Metadata')]\n",
    "        dmeta = data[metadata]\n",
    "        df_scaled = pd.concat([dmeta, df_scaled], axis=1, sort=False)\n",
    "    \n",
    "        pf = aggregate_func(population = df_scaled,\n",
    "                        strata= ['Metadata_Plate', 'Metadata_Well'],\n",
    "                        variables = featlist,\n",
    "                        operation = 'median')\n",
    "        \n",
    "\n",
    "        prf = reduce(lambda x, y: pd.merge(x, y, on = [\"Metadata_Plate\",\"Metadata_Well\"], how='left'),\n",
    "              [pmeta, pf])\n",
    "        \n",
    "        prf.to_csv('/Users/habbasi/Desktop/TA/workspace/backend/SIGMA2_Pilot_2013_10_11/'+str(p)+'_median.csv')\n",
    "        \n",
    "        \n",
    "        combined.append(prf)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return combined\n",
    "    \n",
    "    \n",
    "starttime = time.time()    \n",
    "combined_prf = pd.concat(combined_profiles())    \n",
    "\n",
    "\n",
    "print('That took {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Metadata_mmoles_per_liter' not in combined_prf:\n",
    "    combined_prf['Metadata_mmoles_per_liter'] = int(10)\n",
    "\n",
    "strata = ['Metadata_broad_sample', 'Metadata_mmoles_per_liter', 'Metadata_Plate_Map_Name']\n",
    "profiles = (combined_prf.groupby(strata)[featlist]\n",
    "                .apply(lambda x: np.mean(x))\n",
    "                .reset_index()\n",
    "               ) \n",
    "\n",
    "metadata_df = pd.read_csv(str(metadata_dfpath))\n",
    "prf= pd.merge(profiles, metadata_df, on='Metadata_broad_sample', how='left' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating correlation profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(prf[featlist])\n",
    "upper = np.triu(corr_matrix, k=1)\n",
    "tmp = pd.DataFrame(upper, \n",
    "                   columns=list(prf.Metadata_broad_sample), \n",
    "                   index = list(prf.Metadata_broad_sample))\n",
    "\n",
    "tmp1 = (tmp.stack()\n",
    "        .reset_index())\n",
    "        \n",
    "new_col=['Var1', 'Var2', 'value']\n",
    "tmp1.columns = new_col\n",
    "tmp1 = tmp1.query('value != 0.000000')\n",
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_prf.to_csv('/Users/habbasi/Desktop/TA/workspace/backend/SIGMA2_Pilot_2013_10_11/combined_median.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt = pd.merge(tmp1, metadata_df, left_on='Var1', right_on = 'Metadata_broad_sample', how='left' )\n",
    "melt = pd.merge(melt, metadata_df, left_on='Var2', right_on = 'Metadata_broad_sample', how='left' )\n",
    "melt = melt.loc[:, ['Metadata_moa_x', 'Metadata_moa_y', 'value']]\n",
    "melt[\"Metadata_moa.x\"].map(lambda x: x if type(x)!=str else x.lower())\n",
    "melt[\"Metadata_moa.y\"].map(lambda x: x if type(x)!=str else x.lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
